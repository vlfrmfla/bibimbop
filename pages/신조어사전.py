import streamlit as st
import json
from datetime import datetime
from streamlit_autorefresh import st_autorefresh
import hgtk
import requests


st.set_page_config(layout="wide")

# JSON íŒŒì¼ ê²½ë¡œ
json_file_path = "./new_words.json"

# JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜
def load_data():
    try:
        with open(json_file_path, "r", encoding="utf-8") as file:
            return json.load(file)
    except FileNotFoundError:
        return {}

# JSON íŒŒì¼ì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_data(data):
    with open(json_file_path, "w", encoding="utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=4)

# ì´ˆì„± ë¶„ë¦¬ í•¨ìˆ˜
def extract_initials(word):
    initials = []
    for char in word:
        if hgtk.checker.is_hangul(char):
            try:
                initial = hgtk.letter.decompose(char)[0]
                initials.append(initial)
            except hgtk.exception.NotHangulException:
                initials.append(char)
        else:
            initials.append(char)
    return ''.join(initials)

# ì‹ ì¡°ì–´ ë°ì´í„° ë¡œë“œ
new_words_dict = load_data()

# í˜ì´ì§€ ìë™ ìƒˆë¡œ ê³ ì¹¨ ì„¤ì • (3ì´ˆë§ˆë‹¤ ìƒˆë¡œ ê³ ì¹¨)
count = st_autorefresh(interval=3000, limit=None, key="fizzbuzzcounter")

# Streamlit ì œëª© ì„¤ì •
st.title("ğŸ“ ì‹ ì¡°ì–´ ì‚¬ì „")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'index' not in st.session_state:
    st.session_state.index = 0

# ì‹ ì¡°ì–´ ìŠ¬ë¼ì´ë“œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼
words = list(new_words_dict.items())
if words:
    display_slot = st.empty()

    # í˜„ì¬ ì¸ë±ìŠ¤ì— ë”°ë¼ ì‹ ì¡°ì–´ í‘œì‹œ
    current_batch = words[st.session_state.index:st.session_state.index + 5]
    st.session_state.index = (st.session_state.index + 5) % len(words)

    # ì „ì²´ ëª©ë¡ì„ í•˜ë‚˜ì˜ ë‘¥ê·¼ í…Œë‘ë¦¬ë¡œ ë¬¶ê¸° ìœ„í•œ ìŠ¤íƒ€ì¼
    rounded_box_style = """
    <style>
    .rounded-box {
        border: 2px solid #ddd;
        border-radius: 15px;
        padding: 15px;
        margin-bottom: 20px;
        background-color: white;
        color: black;
    }
    .rounded-box p {
        margin: 0;
        padding: 0;
    }
    </style>
    """
    st.markdown(rounded_box_style, unsafe_allow_html=True)

    # ì‹ ì¡°ì–´ ëª©ë¡ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë‘¥ê·¼ í…Œë‘ë¦¬ë¡œ ë¬¶ìŒ
    with display_slot.container():
        html_content = '<div class="rounded-box">'
        for word, info in current_batch:
            html_content += f"<p><strong>{word}</strong>: {info['definition']} (ì¶”ê°€ëœ ë‚ ì§œ: {info['date']})</p>"
        html_content += '</div>'
        st.markdown(html_content, unsafe_allow_html=True)
else:
    st.write("ì‚¬ì „ì— ì‹ ì¡°ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
# ì‹ ì¡°ì–´ ê²€ìƒ‰ ê¸°ëŠ¥
search_word = st.text_input("ğŸ” ì‹ ì¡°ì–´ ê²€ìƒ‰ (ì´ˆì„± ë° ì¼ë°˜ ê²€ìƒ‰ ì§€ì›)", "")

# ê³ ì •ëœ ìœ íŠœë¸Œ ë§í¬ ë”•ì…”ë„ˆë¦¬ (í‚¤ì›Œë“œì™€ ìœ íŠœë¸Œ ë§í¬ ë§¤ì¹­)
fixed_youtube_links = {
    "ê°“ëµì‘": "https://youtube.com/shorts/fszBquMocC4?si=d2gSp2eZ5xzvHHs0",
    "ì¶”êµ¬ë¯¸": "https://youtube.com/shorts/drUgTiltRpw?si=xGhIyyXRsIqycdXM"
    # ì—¬ê¸°ì— ì¶”ê°€ì ì¸ í‚¤ì›Œë“œì™€ ë§í¬ë¥¼ ë„£ìœ¼ì„¸ìš”
}

# ì¶”ì²œ ê²€ìƒ‰ì–´ ì—…ë°ì´íŠ¸
if search_word:
    # ì´ˆì„±ìœ¼ë¡œ ê²€ìƒ‰
    initial_search_results = {word: info for word, info in new_words_dict.items() if extract_initials(word).startswith(search_word)}

    # ì¼ë°˜ ê²€ìƒ‰
    general_search_results = {word: info for word, info in new_words_dict.items() if search_word in word}

    # ì¶”ì²œ ê²€ìƒ‰ ëª©ë¡
    recommended_search = list(set(initial_search_results.keys()).union(set(general_search_results.keys())))

    # ì¶”ì²œ ê²€ìƒ‰ì–´ê°€ ìˆì„ ê²½ìš°, ì¶”ì²œ ê²€ìƒ‰ì–´ ëª©ë¡ í‘œì‹œ
    if recommended_search:
        st.write("ì¶”ì²œ ê²€ìƒ‰ì–´:")
        for suggestion in recommended_search:
            st.write(suggestion)

    # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°, ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    if initial_search_results or general_search_results:
        st.subheader("ğŸ” ê²€ìƒ‰ ê²°ê³¼")
        combined_results = {**initial_search_results, **general_search_results}
        for word, info in combined_results.items():
            st.markdown(f"**{word}**: {info['definition']} (ì¶”ê°€ëœ ë‚ ì§œ: {info['date']})")
    else:
        st.warning("í•´ë‹¹ ì´ˆì„±ì´ë‚˜ ê²€ìƒ‰ì–´ë¡œ ì‹œì‘í•˜ëŠ” ì‹ ì¡°ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ê³ ì •ëœ ìœ íŠœë¸Œ ë§í¬ í‘œì‹œ
    if search_word in fixed_youtube_links:
        original_video_url = fixed_youtube_links[search_word]
        
        # ìœ íŠœë¸Œ ì‡¼ì¸  ë§í¬ë¥¼ ì¼ë°˜ ìœ íŠœë¸Œ ë§í¬ë¡œ ë³€í™˜
        if "youtube.com/shorts/" in original_video_url:
            video_id = original_video_url.split('/')[-1].split('?')[0]
            video_url = f"https://www.youtube.com/watch?v={video_id}"
        else:
            video_url = original_video_url
        
        st.subheader("ìœ íŠœë¸Œ ë™ì˜ìƒ")
        st.video(video_url)
    else:
        st.write("ê´€ë ¨ ìœ íŠœë¸Œ ë™ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# ì‹ ì¡°ì–´ ì¶”ê°€ ê¸°ëŠ¥
st.subheader("â• ì‹ ì¡°ì–´ ì¶”ê°€í•˜ê¸°")

new_word = st.text_input("ì‹ ì¡°ì–´", "")
new_definition = st.text_area("ì •ì˜", "")

if st.button("ì¶”ê°€"):
    if new_word and new_definition:
        new_words_dict[new_word] = {
            "definition": new_definition,
            "date": datetime.now().strftime("%Y-%m-%d")
        }
        save_data(new_words_dict)  # JSON íŒŒì¼ì— ì €ì¥
        st.success(f"'{new_word}'ì´(ê°€) ì‚¬ì „ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        st.error("ì‹ ì¡°ì–´ì™€ ì •ì˜ë¥¼ ëª¨ë‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

# ì‹ ì¡°ì–´ ëª©ë¡ í‘œì‹œ
st.subheader("ğŸ“š ì‹ ì¡°ì–´ ëª©ë¡")

if new_words_dict:
    for word, info in new_words_dict.items():
        st.markdown(f"**{word}**: {info['definition']} (ì¶”ê°€ëœ ë‚ ì§œ: {info['date']})")
else:
    st.write("ì‚¬ì „ì— ì‹ ì¡°ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")